{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "#######################################################################\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\" Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract(). \"\"\"\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "cifar10_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "download_dir = \"cifar-10/\"\n",
    "data_path = \"cifar-10/\"\n",
    "\n",
    "# Filename for saving the file downloaded from the internet.\n",
    "# Use the filename from the URL and add it to the download_dir.\n",
    "filename = cifar10_url.split('/')[-1]\n",
    "file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "# Check if the file already exists.\n",
    "# If it exists then we assume it has also been extracted,\n",
    "# otherwise we need to download and extract it now.\n",
    "if not os.path.exists(file_path):\n",
    "    # Check if the download directory exists, otherwise create it.\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    # Download and extract the dataset\n",
    "    # Download the file from the internet.\n",
    "    file_path, _ = urllib.request.urlretrieve(url=cifar10_url,\n",
    "                                          filename=file_path,\n",
    "                                          reporthook=_print_download_progress)\n",
    "    print()\n",
    "    print(\"Download finished. Extracting files.\")\n",
    "\n",
    "    if file_path.endswith(\".zip\"):\n",
    "        # Unpack the zip-file.\n",
    "        zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "    elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "        # Unpack the tar-ball.\n",
    "        tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"Data has apparently already been downloaded and unpacked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image shape:  (32, 32, 3)\n",
      "No. of training images:  40000\n",
      "No. of validation images:  10000\n",
      "No. of testing images:  10000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "cifar10_path = os.path.join(data_path, \"cifar-10-batches-py/\")\n",
    "\n",
    "#############################################################################\n",
    "def load_images(file_path):\n",
    "    # Read images from data set\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding, otherwise an exception is raised here.\n",
    "        # Loading the data from pickle file\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "        # get the raw images\n",
    "        raw_images = data[b'data']\n",
    "        # get the class numbers for each image, convert to numpy array\n",
    "        labels = np.array(data[b'labels'])\n",
    "        # Convert the raw images from the data-files to floating-points.\n",
    "        raw_float = np.array(raw_images, dtype=float) / 255.0\n",
    "        # Reshape the array to 4-dimensions.\n",
    "        images = raw_float.reshape([-1, 3, 32, 32])\n",
    "        # Reorder the indices of the array.\n",
    "        images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "#############################################################################\n",
    "def load_data(data_path):\n",
    "    num_images = 10000\n",
    "    x = 0\n",
    "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "    x_train = np.zeros(shape=[40000, 32, 32, 3], dtype=float)\n",
    "    y_train = np.zeros(shape=[40000], dtype=int)\n",
    "    # attach the filename to directory\n",
    "    train_path = os.path.join(data_path, \"data_batch_1\")\n",
    "    x_train[x: x + num_images, :], y_train[x: x + num_images] = load_images(train_path)\n",
    "    x = x + num_images\n",
    "    \n",
    "    train_path = os.path.join(data_path, \"data_batch_2\")\n",
    "    x_train[x: x + num_images, :], y_train[x: x + num_images] = load_images(train_path)\n",
    "    x = x + num_images\n",
    "    \n",
    "    train_path = os.path.join(data_path, \"data_batch_3\")\n",
    "    x_train[x: x + num_images, :], y_train[x: x + num_images] = load_images(train_path)\n",
    "    x = x + num_images\n",
    "    \n",
    "    train_path = os.path.join(data_path, \"data_batch_4\")\n",
    "    x_train[x: x + num_images, :], y_train[x: x + num_images] = load_images(train_path)\n",
    "    \n",
    "    validation_path = os.path.join(data_path, \"data_batch_5\")\n",
    "    x_validation, y_validation = load_images(validation_path)\n",
    "    \n",
    "    test_path = os.path.join(data_path, \"test_batch\")\n",
    "    x_test, y_test = load_images(test_path)\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test    \n",
    "\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = load_data(cifar10_path)\n",
    "\n",
    "# Resize images to 224x224x3 for AlexNet\n",
    "# X_train      = numpy.ndarray(tf.image.resize_images(X_train, [224, 224]))\n",
    "# X_validation = numpy.ndarray(tf.image.resize_images(X_validation, [224, 224]))\n",
    "# X_test       = numpy.ndarray(tf.image.resize_images(X_test, [224, 224]))\n",
    "\n",
    "# Print image data set parameters\n",
    "print('Input Image shape: ', X_train[0].shape)\n",
    "print(\"No. of training images: \", len(X_train))\n",
    "print(\"No. of validation images: \", len(X_validation))\n",
    "print(\"No. of testing images: \", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# select a random image\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "# plot the images\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "\n",
    "# pre-processing \n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement AlexNet in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "epsilon = 1e-3\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def AlexNet(x):    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = . Output = .\n",
    "    # conv1_kernel = tf.Variable(tf.truncated_normal(shape=(11, 11, 3, 96), mean = mu, stddev = sigma))\n",
    "    conv1_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 96), mean = mu, stddev = sigma))\n",
    "    conv1_bias   = tf.Variable(tf.zeros(96))\n",
    "    # conv1_output = tf.nn.conv2d(x, conv1_kernel, strides=[1, 4, 4, 1], padding='SAME') + conv1_bias\n",
    "    conv1_output = tf.nn.conv2d(x, conv1_kernel, strides=[1, 4, 4, 1], padding='SAME') + conv1_bias\n",
    "    # Layer 1: Batch Normalization\n",
    "    conv1_mean, conv1_var = tf.nn.moments(conv1_output, [0])\n",
    "    conv1_scale = tf.Variable(tf.ones([96]))\n",
    "    conv1_beta  = tf.Variable(tf.zeros([96]))\n",
    "    conv1_batch_norm = tf.nn.batch_normalization(conv1_output, conv1_mean, conv1_var, conv1_beta, conv1_scale, epsilon)\n",
    "    # Layer 1: Activation.\n",
    "    conv1_act = tf.nn.relu(conv1_batch_norm)\n",
    "    # Layer 1: Pooling. Input = . Output = .\n",
    "    conv1_pool = tf.nn.max_pool(conv1_act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Input = . Output = .\n",
    "    # conv2_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 96, 256), mean = mu, stddev = sigma))\n",
    "    conv2_kernel = tf.Variable(tf.truncated_normal(shape=(3, 3, 96, 256), mean = mu, stddev = sigma))\n",
    "    conv2_bias   = tf.Variable(tf.zeros(256))\n",
    "    conv2_output = tf.nn.conv2d(conv1_pool, conv2_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias\n",
    "    # Layer 2: Batch Normalization\n",
    "    conv2_mean, conv2_var = tf.nn.moments(conv2_output, [0])\n",
    "    conv2_scale = tf.Variable(tf.ones([256]))\n",
    "    conv2_beta  = tf.Variable(tf.zeros([256]))\n",
    "    conv2_batch_norm = tf.nn.batch_normalization(conv2_output, conv2_mean, conv2_var, conv2_beta, conv2_scale, epsilon)\n",
    "    # Layer 2: Activation.\n",
    "    conv2_act = tf.nn.relu(conv2_batch_norm)\n",
    "    # Layer 2: Pooling. Input = . Output = .\n",
    "    conv2_pool = tf.nn.max_pool(conv2_act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 3: Convolutional. Input = . Output = .\n",
    "    conv3_kernel = tf.Variable(tf.truncated_normal(shape=(3, 3, 256, 384), mean = mu, stddev = sigma))\n",
    "    conv3_bias   = tf.Variable(tf.zeros(384))\n",
    "    conv3_output = tf.nn.conv2d(conv2_pool, conv3_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv3_bias\n",
    "    # Layer 3: Batch Normalization\n",
    "    conv3_mean, conv3_var = tf.nn.moments(conv3_output, [0])\n",
    "    conv3_scale = tf.Variable(tf.ones([384]))\n",
    "    conv3_beta  = tf.Variable(tf.zeros([384]))\n",
    "    conv3_batch_norm = tf.nn.batch_normalization(conv3_output, conv3_mean, conv3_var, conv3_beta, conv3_scale, epsilon)\n",
    "    # Layer 3: Activation.\n",
    "    conv3_act = tf.nn.relu(conv3_batch_norm)\n",
    "\n",
    "    # Layer 4: Convolutional. Input = . Output = .\n",
    "    conv4_kernel = tf.Variable(tf.truncated_normal(shape=(3, 3, 384, 384), mean = mu, stddev = sigma))\n",
    "    conv4_bias   = tf.Variable(tf.zeros(384))\n",
    "    conv4_output = tf.nn.conv2d(conv3_act, conv4_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv4_bias\n",
    "    # Layer 4: Batch Normalization\n",
    "    conv4_mean, conv4_var = tf.nn.moments(conv4_output, [0])\n",
    "    conv4_scale = tf.Variable(tf.ones([384]))\n",
    "    conv4_beta  = tf.Variable(tf.zeros([384]))\n",
    "    conv4_batch_norm = tf.nn.batch_normalization(conv4_output, conv4_mean, conv4_var, conv4_beta, conv4_scale, epsilon)\n",
    "    # Layer 4: Activation.\n",
    "    conv4_act = tf.nn.relu(conv4_batch_norm)\n",
    "\n",
    "    # Layer 5: Convolutional. Input = . Output = .\n",
    "    conv5_kernel = tf.Variable(tf.truncated_normal(shape=(3, 3, 384, 384), mean = mu, stddev = sigma))\n",
    "    conv5_bias   = tf.Variable(tf.zeros(384))\n",
    "    conv5_output = tf.nn.conv2d(conv4_act, conv5_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv5_bias\n",
    "    # Layer 5: Batch Normalization\n",
    "    conv5_mean, conv5_var = tf.nn.moments(conv5_output, [0])\n",
    "    conv5_scale = tf.Variable(tf.ones([384]))\n",
    "    conv5_beta  = tf.Variable(tf.zeros([384]))\n",
    "    conv5_batch_norm = tf.nn.batch_normalization(conv5_output, conv5_mean, conv5_var, conv5_beta, conv5_scale, epsilon)\n",
    "    # Layer 5: Activation.\n",
    "    conv5_act = tf.nn.relu(conv5_batch_norm)\n",
    "    # Layer 5: Pooling. Input = . Output = .\n",
    "    conv5_pool = tf.nn.max_pool(conv5_act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')    \n",
    "    \n",
    "    # Layer 6: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv5_pool)\n",
    "    \n",
    "    # Layer 7: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_kernel = tf.Variable(tf.truncated_normal(shape=(384, 120), mean = mu, stddev = sigma))\n",
    "    fc1_bias   = tf.Variable(tf.zeros(120))\n",
    "    fc1_output = tf.matmul(fc0, fc1_kernel) + fc1_bias\n",
    "    # Layer 7: Batch Normalization\n",
    "    fc1_mean, fc1_var = tf.nn.moments(fc1_output, [0])\n",
    "    fc1_scale = tf.Variable(tf.ones([120]))\n",
    "    fc1_beta  = tf.Variable(tf.zeros([120]))\n",
    "    fc1_batch_norm = tf.nn.batch_normalization(fc1_output, fc1_mean, fc1_var, fc1_beta, fc1_scale, epsilon)    \n",
    "    # Layer 7: Activation.\n",
    "    fc1_act = tf.nn.relu(fc1_batch_norm)\n",
    "    # Layer 7: Drop out\n",
    "    fc2_act = tf.nn.dropout(fc1_act, 0.4) \n",
    "    \n",
    "    # Layer 8: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_kernel = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_bias   = tf.Variable(tf.zeros(84))\n",
    "    fc2_output = tf.matmul(fc1_act, fc2_kernel) + fc2_bias\n",
    "    # Layer 8: Batch Normalization\n",
    "    fc2_mean, fc2_var = tf.nn.moments(fc2_output, [0])\n",
    "    fc2_scale = tf.Variable(tf.ones([84]))\n",
    "    fc2_beta  = tf.Variable(tf.zeros([84]))\n",
    "    fc2_batch_norm = tf.nn.batch_normalization(fc2_output, fc2_mean, fc2_var, fc2_beta, fc2_scale, epsilon)        \n",
    "    # Layer 8: Activation.\n",
    "    fc2_act = tf.nn.relu(fc2_batch_norm)\n",
    "    # Layer 8: Drop out \n",
    "    fc2_act = tf.nn.dropout(fc2_act, 0.4) \n",
    "    \n",
    "    # Layer 9: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_kernel = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_bias   = tf.Variable(tf.zeros(10))\n",
    "    logits     = tf.matmul(fc2_act, fc3_kernel) + fc3_bias\n",
    "    \n",
    "    return logits\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = AlexNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Model evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n",
    "\n",
    "Note: Saves four files\n",
    "\n",
    "1. checkpoint : \n",
    "2. lenet.data-0000-of-00001 : holding weights\n",
    "3. lenet.index\n",
    "4. lenet.meta : graph meta data for re-training\n",
    "\n",
    "Also, saves frozen model as *.pb (binary) and *.pbtxt (human readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.488\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'model/AlexNet')\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/AlexNet.pbtxt', as_text=True)\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/AlexNet.pb', as_text=False)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/AlexNet\n",
      "Test Accuracy = 0.493\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
