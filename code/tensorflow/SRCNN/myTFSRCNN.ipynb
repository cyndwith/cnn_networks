{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image \n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# Input setup\n",
    "# prepare data\n",
    "dataset = \"Data\"\n",
    "filenames = os.listdir(dataset) # print(filenames)\n",
    "data_dir  = os.path.join(os.getcwd(), dataset) # print(data_dir)\n",
    "data      = glob.glob(os.path.join(data_dir,\"*.bmp\")) # print(data)\n",
    "sub_input_sequence = []\n",
    "sub_label_sequence = []\n",
    "# Loop through the images\n",
    "for i in range(len(data)):\n",
    "    image = imread(data[i])  # imgplot = plt.imshow(image / 255.)\n",
    "    scale = 3\n",
    "    h, w, _ = image.shape\n",
    "    h = h - np.mod(h, scale)\n",
    "    w = w - np.mod(w, scale)\n",
    "    label = image[0:h, 0:w, :]\n",
    "    \n",
    "    image = image / 255.\n",
    "    label = label / 255.\n",
    "\n",
    "    input_ = scipy.ndimage.interpolation.zoom(label, (1./scale), prefilter=False)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(input_, (scale/1.), prefilter=False)    \n",
    "    \n",
    "    stride = 14\n",
    "    image_size = 33\n",
    "    label_size = 21\n",
    "    pad = abs(image_size - label_size) / 2\n",
    "    for x in range(0, h - image_size+1, stride):\n",
    "        for y in range(0, w - image_size+1, stride):\n",
    "            sub_input = input_[x:x+image_size, y:y+image_size, 0]\n",
    "            sub_label = label[x+int(pad):x+int(pad)+label_size, y+int(pad):y+int(pad)+label_size, 0]\n",
    "            sub_input = sub_input.reshape([image_size, image_size, 1])\n",
    "            sub_label = sub_label.reshape([label_size, label_size, 1])\n",
    "            \n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "arrData = np.asarray(sub_input_sequence)\n",
    "arrLabel = np.asarray(sub_label_sequence)\n",
    "savepath = os.path.join(os.getcwd(), 'checkpoint\\\\train.h5')\n",
    "with h5py.File(savepath, 'w') as hf:\n",
    "    hf.create_dataset('data', data=arrData)\n",
    "    hf.create_dataset('label', data=arrLabel) \n",
    "\n",
    "X_train = arrData;\n",
    "y_train = arrLabel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "def SRCNN(input_image): #9-5-5\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # Layer 1: Convolution Input =. Output=.\n",
    "    conv1_kernel = tf.Variable(tf.truncated_normal(shape=(9, 9, 1, 64), mean = mu, stddev = sigma))\n",
    "    conv1_bias   = tf.Variable(tf.zeros(64))\n",
    "    conv1_output = tf.nn.conv2d(input_image, conv1_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
    "    conv1_act = tf.nn.relu(conv1_output)\n",
    "    # Layer 2: Convolution Input =. Output =.\n",
    "    conv2_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 32), mean = mu, stddev = sigma))\n",
    "    conv2_bias   = tf.Variable(tf.zeros(32))\n",
    "    conv2_output = tf.nn.conv2d(conv1_act, conv2_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias\n",
    "    conv2_act = tf.nn.relu(conv2_output)\n",
    "    # Layer 3: Convolution Input =. Output =.\n",
    "    conv3_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 1), mean = mu, stddev = sigma))\n",
    "    conv3_bias   = tf.Variable(tf.zeros(1))\n",
    "    conv3_output = tf.nn.conv2d(conv2_act, conv3_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv3_bias\n",
    "    conv3_act = tf.nn.relu(conv3_output)\n",
    "    \n",
    "    return conv3_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert training pipeline\n",
    "input_image = tf.placeholder(tf.float32, [None, 33, 33, 1], name='input_image')\n",
    "labels = tf.placeholder(tf.float32, [None, 21, 21, 1], name='labels')\n",
    "output = SRCNN(input_image)\n",
    "loss = tf.reduce_mean(tf.square(labels - output))\n",
    "training_operation = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/SRCNN.ckpt\n",
      "Loaded model from disk\n",
      "Training...\n",
      "\n",
      "Epoch: [ 1], step: [10], loss: [0.01257494]\n",
      "Epoch: [ 1], step: [20], loss: [0.04822126]\n",
      "Epoch: [ 1], step: [30], loss: [0.00636762]\n",
      "Epoch: [ 1], step: [40], loss: [0.01790082]\n",
      "Epoch: [ 1], step: [50], loss: [0.03573802]\n",
      "Epoch: [ 1], step: [60], loss: [0.00779900]\n",
      "Epoch: [ 1], step: [70], loss: [0.04311228]\n",
      "Epoch: [ 1], step: [80], loss: [0.00295816]\n",
      "Epoch: [ 1], step: [90], loss: [0.01497218]\n",
      "Epoch: [ 1], step: [100], loss: [0.02255728]\n",
      "Epoch: [ 1], step: [110], loss: [0.00232057]\n",
      "Epoch: [ 1], step: [120], loss: [0.01677324]\n",
      "Epoch: [ 1], step: [130], loss: [0.02664786]\n",
      "Epoch: [ 1], step: [140], loss: [0.00590266]\n",
      "Epoch: [ 1], step: [150], loss: [0.01099588]\n",
      "Epoch: [ 1], step: [160], loss: [0.03102288]\n",
      "Epoch: [ 1], step: [170], loss: [0.00746466]\n",
      "EPOCH 1 ...\n",
      "Error rate = 0.006\n",
      "\n",
      "Model saved\n",
      "Epoch: [ 2], step: [180], loss: [0.00928154]\n",
      "Epoch: [ 2], step: [190], loss: [0.02496022]\n",
      "Epoch: [ 2], step: [200], loss: [0.01410281]\n",
      "Epoch: [ 2], step: [210], loss: [0.04003707]\n",
      "Epoch: [ 2], step: [220], loss: [0.00476467]\n",
      "Epoch: [ 2], step: [230], loss: [0.00720563]\n",
      "Epoch: [ 2], step: [240], loss: [0.01379165]\n",
      "Epoch: [ 2], step: [250], loss: [0.00388280]\n",
      "Epoch: [ 2], step: [260], loss: [0.02502454]\n",
      "Epoch: [ 2], step: [270], loss: [0.02638412]\n",
      "Epoch: [ 2], step: [280], loss: [0.00385949]\n",
      "Epoch: [ 2], step: [290], loss: [0.00809071]\n",
      "Epoch: [ 2], step: [300], loss: [0.01305128]\n",
      "Epoch: [ 2], step: [310], loss: [0.00585492]\n",
      "Epoch: [ 2], step: [320], loss: [0.00554233]\n",
      "Epoch: [ 2], step: [330], loss: [0.02350628]\n",
      "Epoch: [ 2], step: [340], loss: [0.00610961]\n",
      "EPOCH 2 ...\n",
      "Error rate = 0.006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from pathlib import Path\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    model_file = Path(\"model/SRCNN.ckpt.meta\")\n",
    "    if model_file.is_file():\n",
    "        saver.restore(sess, 'model/SRCNN.ckpt')\n",
    "        print(\"Loaded model from disk\") \n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    counter = 0\n",
    "    for i in range(EPOCHS):\n",
    "        # X_train = tf.random.shuffle(X_train, 1)\n",
    "        # y_train = tf.random.shuffle(y_train, 1)\n",
    "        Error = 0\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            counter = counter + 1\n",
    "            _, err = sess.run([training_operation, loss], feed_dict={input_image: batch_x, labels: batch_y})\n",
    "            Error = err\n",
    "            if counter % 10 == 0:\n",
    "                print(\"Epoch: [%2d], step: [%2d], loss: [%.8f]\" % ((i+1), counter, err))\n",
    "        \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Error rate = {:.3f}\".format(err))\n",
    "        print()\n",
    "        \n",
    "        saver.save(sess, 'model/SRCNN.ckpt')\n",
    "        tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pbtxt', as_text=True)\n",
    "        tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pb', as_text=False)\n",
    "        print(\"Model saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
