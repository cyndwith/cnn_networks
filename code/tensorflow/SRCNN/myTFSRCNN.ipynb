{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image \n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# Input setup\n",
    "# prepare data\n",
    "dataset = \"Data\"\n",
    "filenames = os.listdir(dataset) # print(filenames)\n",
    "data_dir  = os.path.join(os.getcwd(), dataset) # print(data_dir)\n",
    "data      = glob.glob(os.path.join(data_dir,\"*.bmp\")) # print(data)\n",
    "sub_input_sequence = []\n",
    "sub_label_sequence = []\n",
    "# Loop through the images\n",
    "for i in range(len(data)):\n",
    "    image = imageio.imread(data[i]).astype(np.float)\n",
    "    #image = scipy.misc.imread(data[i], mode='YCbCr').astype(np.float)\n",
    "    #image = imread(data[i])  # imgplot = plt.imshow(image / 255.)\n",
    "    scale = 3\n",
    "    h, w, _ = image.shape\n",
    "    h = h - np.mod(h, scale)\n",
    "    w = w - np.mod(w, scale)\n",
    "    label = image[0:h, 0:w, :]\n",
    "    \n",
    "    image = image / 255.\n",
    "    label = label / 255.\n",
    "\n",
    "    input_ = scipy.ndimage.interpolation.zoom(label, (1./scale), prefilter=False)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(input_, (scale/1.), prefilter=False)    \n",
    "    \n",
    "    stride = 14\n",
    "    image_size = 33\n",
    "    label_size = 21\n",
    "    pad = abs(image_size - label_size) / 2\n",
    "    for x in range(0, h - image_size+1, stride):\n",
    "        for y in range(0, w - image_size+1, stride):\n",
    "            sub_input = input_[x:x+image_size, y:y+image_size, 0]\n",
    "            sub_label = label[x+int(pad):x+int(pad)+label_size, y+int(pad):y+int(pad)+label_size, 0]\n",
    "            sub_input = sub_input.reshape([image_size, image_size, 1])\n",
    "            sub_label = sub_label.reshape([label_size, label_size, 1])\n",
    "            \n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "arrData = np.asarray(sub_input_sequence)\n",
    "arrLabel = np.asarray(sub_label_sequence)\n",
    "savepath = os.path.join(os.getcwd(), 'checkpoint\\\\train.h5')\n",
    "with h5py.File(savepath, 'w') as hf:\n",
    "    hf.create_dataset('data', data=arrData)\n",
    "    hf.create_dataset('label', data=arrLabel) \n",
    "\n",
    "X_train = arrData;\n",
    "y_train = arrLabel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 1500\n",
    "BATCH_SIZE = 128\n",
    "def SRCNN(input_image): #9-5-5\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # Layer 1: Convolution Input =. Output=.\n",
    "    conv1_kernel = tf.Variable(tf.truncated_normal(shape=(9, 9, 1, 64), mean = mu, stddev = sigma))\n",
    "    conv1_bias   = tf.Variable(tf.zeros(64))\n",
    "    conv1_output = tf.nn.conv2d(input_image, conv1_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
    "    conv1_act = tf.nn.relu(conv1_output)\n",
    "    # Layer 2: Convolution Input =. Output =.\n",
    "    conv2_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 32), mean = mu, stddev = sigma))\n",
    "    conv2_bias   = tf.Variable(tf.zeros(32))\n",
    "    conv2_output = tf.nn.conv2d(conv1_act, conv2_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias\n",
    "    conv2_act = tf.nn.relu(conv2_output)\n",
    "    # Layer 3: Convolution Input =. Output =.\n",
    "    conv3_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 1), mean = mu, stddev = sigma))\n",
    "    conv3_bias   = tf.Variable(tf.zeros(1))\n",
    "    conv3_output = tf.nn.conv2d(conv2_act, conv3_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv3_bias\n",
    "    conv3_act = tf.nn.relu(conv3_output)\n",
    "    \n",
    "    return conv3_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert training pipeline\n",
    "input_image = tf.placeholder(tf.float32, [None, 33, 33, 1], name='input_image')\n",
    "labels = tf.placeholder(tf.float32, [None, 21, 21, 1], name='labels')\n",
    "output = SRCNN(input_image)\n",
    "loss = tf.reduce_mean(tf.square(labels - output))\n",
    "training_operation = tf.train.GradientDescentOptimizer(learning_rate = 0.0001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Epoch: [ 1], step: [10], loss: [0.14234193]\n",
      "Epoch: [ 1], step: [20], loss: [0.12261878]\n",
      "Epoch: [ 1], step: [30], loss: [0.09283951]\n",
      "Epoch: [ 1], step: [40], loss: [0.13785802]\n",
      "Epoch: [ 1], step: [50], loss: [0.11991202]\n",
      "Epoch: [ 1], step: [60], loss: [0.07196879]\n",
      "Epoch: [ 1], step: [70], loss: [0.14545460]\n",
      "Epoch: [ 1], step: [80], loss: [0.05851944]\n",
      "Epoch: [ 1], step: [90], loss: [0.07959670]\n",
      "Epoch: [ 1], step: [100], loss: [0.10954367]\n",
      "Epoch: [ 1], step: [110], loss: [0.04556477]\n",
      "Epoch: [ 1], step: [120], loss: [0.06691538]\n",
      "Epoch: [ 1], step: [130], loss: [0.12221613]\n",
      "Epoch: [ 1], step: [140], loss: [0.06523453]\n",
      "Epoch: [ 1], step: [150], loss: [0.07307412]\n",
      "Epoch: [ 1], step: [160], loss: [0.13125998]\n",
      "Epoch: [ 1], step: [170], loss: [0.05763770]\n",
      "EPOCH 1 ...\n",
      "Error rate = 0.05636177\n",
      "\n",
      "Epoch: [ 2], step: [180], loss: [0.07178632]\n",
      "Epoch: [ 2], step: [190], loss: [0.09836701]\n",
      "Epoch: [ 2], step: [200], loss: [0.06385397]\n",
      "Epoch: [ 2], step: [210], loss: [0.07876655]\n",
      "Epoch: [ 2], step: [220], loss: [0.06759430]\n",
      "Epoch: [ 2], step: [230], loss: [0.03216556]\n",
      "Epoch: [ 2], step: [240], loss: [0.07459006]\n",
      "Epoch: [ 2], step: [250], loss: [0.03756489]\n",
      "Epoch: [ 2], step: [260], loss: [0.06775597]\n",
      "Epoch: [ 2], step: [270], loss: [0.10211162]\n",
      "Epoch: [ 2], step: [280], loss: [0.03481304]\n",
      "Epoch: [ 2], step: [290], loss: [0.02155331]\n",
      "Epoch: [ 2], step: [300], loss: [0.05490055]\n",
      "Epoch: [ 2], step: [310], loss: [0.05012619]\n",
      "Epoch: [ 2], step: [320], loss: [0.04872687]\n",
      "Epoch: [ 2], step: [330], loss: [0.07688984]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from pathlib import Path\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    model_file = Path(\"model/SRCNN.ckpt.meta\")\n",
    "    if model_file.is_file():\n",
    "        saver.restore(sess, 'model/SRCNN.ckpt')\n",
    "        print(\"Loaded model from disk\") \n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    counter = 0\n",
    "    for i in range(EPOCHS):\n",
    "        Error = 0\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            counter = counter + 1\n",
    "            _, err = sess.run([training_operation, loss], feed_dict={input_image: batch_x, labels: batch_y})\n",
    "            Error = err\n",
    "            if counter % 10 == 0:\n",
    "                print(\"Epoch: [%2d], step: [%2d], loss: [%.8f]\" % ((i+1), counter, err))\n",
    "            \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Error rate = {:.8f}\".format(Error))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'model/SRCNN.ckpt')\n",
    "    # tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pbtxt', as_text=True)\n",
    "    # tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pb', as_text=False)\n",
    "    print(\"Model saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby_GT.bmp', 'test_image.png']\n",
      "C:\\Users\\dwith\\Documents\\Github\\cnn_networks\\code\\tensorflow\\SRCNN\\Test\n",
      "['C:\\\\Users\\\\dwith\\\\Documents\\\\Github\\\\cnn_networks\\\\code\\\\tensorflow\\\\SRCNN\\\\Test\\\\baby_GT.bmp']\n",
      "(1225, 33, 33, 1)\n",
      "(1225, 21, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "# Setup model evaluation pipeline\n",
    "# Input Test setup\n",
    "# prepare data\n",
    "dataset = \"Test\"\n",
    "filenames = os.listdir(dataset) \n",
    "print(filenames)\n",
    "data_dir  = os.path.join(os.getcwd(), dataset) \n",
    "print(data_dir)\n",
    "data      = glob.glob(os.path.join(data_dir,\"*.bmp\")) \n",
    "print(data)\n",
    "sub_input_sequence = []\n",
    "sub_label_sequence = []\n",
    "# Loop through the images\n",
    "for i in range(len(data)):\n",
    "    image = imageio.imread(data[i]).astype(np.float)\n",
    "    #image = imread(data[i])  # imgplot = plt.imshow(image / 255.)\n",
    "    scale = 3\n",
    "    h, w, _ = image.shape\n",
    "    h = h - np.mod(h, scale)\n",
    "    w = w - np.mod(w, scale)\n",
    "    label = image[0:h, 0:w, :]\n",
    "    \n",
    "    image = image / 255.\n",
    "    label = label / 255.\n",
    "\n",
    "    input_ = scipy.ndimage.interpolation.zoom(label, (1./scale), prefilter=False)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(input_, (scale/1.), prefilter=False)    \n",
    "    \n",
    "    stride = 14\n",
    "    image_size = 33\n",
    "    label_size = 21\n",
    "    pad = abs(image_size - label_size) / 2\n",
    "    \n",
    "    nx = ny = 0\n",
    "    for x in range(0, h - image_size+1, stride):\n",
    "        nx += 1\n",
    "        ny = 0\n",
    "        for y in range(0, w - image_size+1, stride):\n",
    "            ny += 1\n",
    "            sub_input = input_[x:x+image_size, y:y+image_size, 0]\n",
    "            sub_label = label[x+int(pad):x+int(pad)+label_size, y+int(pad):y+int(pad)+label_size, 0]\n",
    "            sub_input = sub_input.reshape([image_size, image_size, 1])\n",
    "            sub_label = sub_label.reshape([label_size, label_size, 1])\n",
    "            \n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "arrData = np.asarray(sub_input_sequence)\n",
    "arrLabel = np.asarray(sub_label_sequence)\n",
    "savepath = os.path.join(os.getcwd(), 'checkpoint\\\\test.h5')\n",
    "with h5py.File(savepath, 'w') as hf:\n",
    "    hf.create_dataset('data', data=arrData)\n",
    "    hf.create_dataset('label', data=arrLabel) \n",
    "\n",
    "X_test = arrData;\n",
    "y_test = arrLabel;\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from model/SRCNN.ckpt\n",
      "Loaded model from disk\n",
      "float64\n",
      "float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [0.0, 1.9312325716018677]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# Insert training pipeline\n",
    "import imageio\n",
    "\n",
    "from pathlib import Path\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h*size[0], w*size[1], 1))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Testing...\")\n",
    "    model_file = Path(\"model/SRCNN.ckpt.meta\")\n",
    "    if model_file.is_file():\n",
    "        saver.restore(sess, 'model/SRCNN.ckpt')\n",
    "        print(\"Loaded model from disk\") \n",
    "    \n",
    "    result = sess.run(output, feed_dict={input_image: X_test, labels: y_test})\n",
    "    result = merge(result, [nx, ny])\n",
    "    result = result.squeeze()\n",
    "    image_path = os.path.join(os.getcwd(), dataset)\n",
    "    image_path = os.path.join(image_path, \"test_image.png\")\n",
    "    # scipy.misc.imsave(image_path, result)\n",
    "    imageio.imsave(image_path, result)\n",
    "    # imsave(result, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
