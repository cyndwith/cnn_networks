{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image \n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# Input setup\n",
    "# prepare data\n",
    "dataset = \"Data\"\n",
    "filenames = os.listdir(dataset) # print(filenames)\n",
    "data_dir  = os.path.join(os.getcwd(), dataset) # print(data_dir)\n",
    "data      = glob.glob(os.path.join(data_dir,\"*.bmp\")) # print(data)\n",
    "sub_input_sequence = []\n",
    "sub_label_sequence = []\n",
    "# Loop through the images\n",
    "for i in range(len(data)):\n",
    "    image = imread(data[i])  # imgplot = plt.imshow(image / 255.)\n",
    "    scale = 3\n",
    "    h, w, _ = image.shape\n",
    "    h = h - np.mod(h, scale)\n",
    "    w = w - np.mod(w, scale)\n",
    "    label = image[0:h, 0:w, :]\n",
    "    \n",
    "    image = image / 255.\n",
    "    label = label / 255.\n",
    "\n",
    "    input_ = scipy.ndimage.interpolation.zoom(label, (1./scale), prefilter=False)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(input_, (scale/1.), prefilter=False)    \n",
    "    \n",
    "    stride = 14\n",
    "    image_size = 33\n",
    "    label_size = 21\n",
    "    pad = abs(image_size - label_size) / 2\n",
    "    for x in range(0, h - image_size+1, stride):\n",
    "        for y in range(0, w - image_size+1, stride):\n",
    "            sub_input = input_[x:x+image_size, y:y+image_size, 0]\n",
    "            sub_label = label[x+int(pad):x+int(pad)+label_size, y+int(pad):y+int(pad)+label_size, 0]\n",
    "            sub_input = sub_input.reshape([image_size, image_size, 1])\n",
    "            sub_label = sub_label.reshape([label_size, label_size, 1])\n",
    "            \n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "arrData = np.asarray(sub_input_sequence)\n",
    "arrLabel = np.asarray(sub_label_sequence)\n",
    "savepath = os.path.join(os.getcwd(), 'checkpoint\\\\train.h5')\n",
    "with h5py.File(savepath, 'w') as hf:\n",
    "    hf.create_dataset('data', data=arrData)\n",
    "    hf.create_dataset('label', data=arrLabel) \n",
    "\n",
    "X_train = arrData;\n",
    "y_train = arrLabel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "def SRCNN(input_image): #9-5-5\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # Layer 1: Convolution Input =. Output=.\n",
    "    conv1_kernel = tf.Variable(tf.truncated_normal(shape=(9, 9, 1, 64), mean = mu, stddev = sigma))\n",
    "    conv1_bias   = tf.Variable(tf.zeros(64))\n",
    "    conv1_output = tf.nn.conv2d(input_image, conv1_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
    "    conv1_act = tf.nn.relu(conv1_output)\n",
    "    # Layer 2: Convolution Input =. Output =.\n",
    "    conv2_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 32), mean = mu, stddev = sigma))\n",
    "    conv2_bias   = tf.Variable(tf.zeros(32))\n",
    "    conv2_output = tf.nn.conv2d(conv1_act, conv2_kernel, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias\n",
    "    conv2_act = tf.nn.relu(conv2_output)\n",
    "    # Layer 3: Convolution Input =. Output =.\n",
    "    conv3_kernel = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 1), mean = mu, stddev = sigma))\n",
    "    conv3_bias   = tf.Variable(tf.zeros(1))\n",
    "    conv3_output = tf.nn.conv2d(conv2_act, conv3_kernel, strides=[1, 1, 1, 1], padding='VALID') + conv3_bias\n",
    "    conv3_act = tf.nn.relu(conv3_output)\n",
    "    \n",
    "    return conv3_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert training pipeline\n",
    "input_image = tf.placeholder(tf.float32, [None, 33, 33, 1], name='input_image')\n",
    "labels = tf.placeholder(tf.float32, [None, 21, 21, 1], name='labels')\n",
    "output = SRCNN(input_image)\n",
    "loss = tf.reduce_mean(tf.square(labels - output))\n",
    "training_operation = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Tensor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dc6da959fa58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnum_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model/SRCNN.ckpt.meta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Tensor' has no len()"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from pathlib import Path\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    model_file = Path(\"model/SRCNN.ckpt.meta\")\n",
    "    if model_file.is_file():\n",
    "        saver.restore(sess, 'model/SRCNN.ckpt')\n",
    "        print(\"Loaded model from disk\") \n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    counter = 0\n",
    "    for i in range(EPOCHS):\n",
    "        # X_train = tf.random.shuffle(X_train, 1)\n",
    "        # y_train = tf.random.shuffle(y_train, 1)\n",
    "        Error = 0\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            counter = counter + 1\n",
    "            _, err = sess.run([training_operation, loss], feed_dict={input_image: batch_x, labels: batch_y})\n",
    "            Error = err\n",
    "            if counter % 10 == 0:\n",
    "                print(\"Epoch: [%2d], step: [%2d], loss: [%.8f]\" % ((i+1), counter, err))\n",
    "        \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Error rate = {:.3f}\".format(err))\n",
    "        print()\n",
    "        \n",
    "        saver.save(sess, 'model/SRCNN.ckpt')\n",
    "        tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pbtxt', as_text=True)\n",
    "        tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model/SRCNN.pb', as_text=False)\n",
    "        print(\"Model saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
