{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import librarries needed for LeNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import neccessary libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MNIST Data set available in keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 7s 1us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 No. of training images\n",
      "10000 No. of test images\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# convert type to float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'No. of training images')\n",
    "print(x_test.shape[0], 'No. of test images')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using keras commands\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.2568 - acc: 0.9207 - val_loss: 0.0552 - val_acc: 0.9821\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0867 - acc: 0.9738 - val_loss: 0.0399 - val_acc: 0.9865\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0636 - acc: 0.9810 - val_loss: 0.0314 - val_acc: 0.9888\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0548 - acc: 0.9834 - val_loss: 0.0303 - val_acc: 0.9900\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0472 - acc: 0.9860 - val_loss: 0.0366 - val_acc: 0.9880\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0420 - acc: 0.9874 - val_loss: 0.0294 - val_acc: 0.9905\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 116s 2ms/step - loss: 0.0372 - acc: 0.9891 - val_loss: 0.0254 - val_acc: 0.9921\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.0348 - acc: 0.9889 - val_loss: 0.0284 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.0307 - acc: 0.9903 - val_loss: 0.0318 - val_acc: 0.9900\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0309 - acc: 0.9903 - val_loss: 0.0269 - val_acc: 0.9910\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.0289 - acc: 0.9916 - val_loss: 0.0259 - val_acc: 0.9920\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0263 - acc: 0.9920 - val_loss: 0.0255 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18eed615630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02549291779803043\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model as :\n",
    "1. Weights : h5\n",
    "2. Architecture : json\n",
    "\n",
    "Load the saved files and evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "Test loss: 0.02549291779803043\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "lenet_model_json = model.to_json()\n",
    "with open(\"model/lenet_model.json\", \"w\") as json_file:\n",
    "    json_file.write(lenet_model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model/lenet_model_weights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load the saved model\n",
    "lenet_model_file = open('model/lenet_model.json', 'r')\n",
    "lenet_model = lenet_model_file.read()\n",
    "lenet_model_file.close()\n",
    "lenet_model = model_from_json(lenet_model)\n",
    "\n",
    "# load weights into new model\n",
    "lenet_model.load_weights(\"model/lenet_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "#evaluate the loaded model\n",
    "lenet_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model as:\n",
    "1. Weights : h5\n",
    "2. Architecture : YAML\n",
    "\n",
    "Load the saved file and evaluate accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "Test loss: 0.02549291779803043\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "# serialize model to JSON\n",
    "lenet_model_yaml = model.to_yaml()\n",
    "with open(\"model/lenet_model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(lenet_model_yaml)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model/lenet_model_weights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load the saved model\n",
    "lenet_model_file = open('model/lenet_model.yaml', 'r')\n",
    "lenet_model = lenet_model_file.read()\n",
    "lenet_model_file.close()\n",
    "lenet_model = model_from_yaml(lenet_model)\n",
    "\n",
    "# load weights into new model\n",
    "lenet_model.load_weights(\"model/lenet_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "#evaluate the loaded model\n",
    "lenet_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
