{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "def PREPARE_DATASET(batch_size):\n",
    "    hr_images_path = 'Data\\DIV2K_train_HR\\*'\n",
    "    hr_images_path = glob(hr_images_path)\n",
    "    \n",
    "    lr_images_ds = []\n",
    "    hr_images_ds = []\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        img_path = np.random.choice(hr_images_path, size=batch_size)\n",
    "        img = imageio.imread(img_path[0])\n",
    "        \n",
    "        hr_patch = scipy.misc.imresize(img, (256, 256))\n",
    "        lr_patch = scipy.misc.imresize(img, (64, 64))\n",
    "\n",
    "        lr_images_ds.append(lr_patch)\n",
    "        hr_images_ds.append(hr_patch)\n",
    "    \n",
    "    lr_images_ds = np.array(lr_images_ds)/127.5 - 1\n",
    "    hr_images_ds = np.array(hr_images_ds)/127.5 - 1\n",
    "    \n",
    "    return hr_images_ds, lr_images_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow.layers as tfl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def SRGAN_GEN(input_shape):\n",
    "    n0 = Input(input_shape)\n",
    "    n = Conv2D(64, kernel_size=9, strides=1, padding='SAME')(n0)\n",
    "    n = Activation('relu')(n)\n",
    "    tmp = n\n",
    "    \n",
    "    # Residual blocks\n",
    "    for i in range(16):\n",
    "        nn = Conv2D(64, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "        nn = Activation('relu')(n)\n",
    "        nn = BatchNormalization(momentum=0.8)(nn)\n",
    "        nn = Conv2D(64, kernel_size=3, strides=1, padding='SAME')(nn)\n",
    "        nn = BatchNormalization(momentum=0.8)(n)\n",
    "        nn = Add()([n, nn])\n",
    "        n  = nn\n",
    "    \n",
    "    n = Conv2D(64, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    n = Add()([n, nn])\n",
    "    \n",
    "    n = UpSampling2D(size=2)(n)\n",
    "    n = Conv2D(256, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "    n = Activation('relu')(n)\n",
    "    \n",
    "    n = UpSampling2D(size=2)(n)\n",
    "    n = Conv2D(256, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "    n = Activation('relu')(n)\n",
    "    \n",
    "    nn = Conv2D(3, kernel_size=9, strides=1, padding='SAME')(n)\n",
    "    \n",
    "    GEN = Model(inputs=n0, outputs=nn, name=\"SRGAN_GENERATOR\")\n",
    "    \n",
    "    return GEN\n",
    "\n",
    "def SRGAN_DIS(input_shape):\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "    \n",
    "    n0 = Input(input_shape)\n",
    "    n = Conv2D(64, kernel_size=4, strides=1, padding='SAME')(n0)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 2, kernel_size=4, strides=2, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = Conv2D(64 * 4, kernel_size=4, strides=2, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    n = Conv2D(64 * 8, kernel_size=4, strides=2, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 16, kernel_size=4, strides=2, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.2)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 32, kernel_size=4, strides=2, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 16, kernel_size=1, strides=1, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 8, kernel_size=1, strides=1, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    nn = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    \n",
    "    n = Conv2D(64 * 2, kernel_size=1, strides=1, padding='SAME')(nn)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 2, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Conv2D(64 * 8, kernel_size=3, strides=1, padding='SAME')(n)\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    n = BatchNormalization(momentum=0.8)(n)\n",
    "    \n",
    "    n = Add()([n, nn])\n",
    "    n = LeakyReLU(alpha=0.2)(n)\n",
    "    \n",
    "    no = Dense(1)(n)\n",
    "    \n",
    "    DIS = Model(inputs=n0, outputs=no, name=\"SRGAN_DISCRIMINATOR\")\n",
    "    \n",
    "    return DIS\n",
    "\n",
    "\n",
    "def VGG_FEATURES(input_shape):\n",
    "    vgg = VGG19(weights=\"imagenet\")\n",
    "    # Set outputs to outputs of last conv. layer in block 3\n",
    "    # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "    vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "    img = Input(input_shape)\n",
    "\n",
    "    # Extract image features\n",
    "    img_features = vgg(img)\n",
    "    \n",
    "    VGG_FEATURES = Model(inputs=img, outputs=img_features, name=\"VGG_FEATURES\")\n",
    "    \n",
    "    return VGG_FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg feature (out metric names): ['loss', 'acc']\n",
      "srgan dis (out metric names): ['loss', 'acc']\n",
      "srgan complete (out metric names): ['loss', 'SRGAN_DISCRIMINATOR_loss', 'VGG_FEATURES_loss']\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwith\\AppData\\Local\\Continuum\\miniconda3\\envs\\cynCNN\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\Users\\dwith\\AppData\\Local\\Continuum\\miniconda3\\envs\\cynCNN\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\Users\\dwith\\AppData\\Local\\Continuum\\miniconda3\\envs\\cynCNN\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dis_loss: [6.7206964  0.16015625]\n",
      "epoch: 0, srgan_loss:  6888.405\n",
      "1\n",
      "dis_loss: [54.79203     0.06640625]\n",
      "epoch: 1, srgan_loss:  2309282.0\n",
      "2\n",
      "dis_loss: [1.3719078e+02 4.2968750e-02]\n",
      "epoch: 2, srgan_loss:  2546.2708\n",
      "3\n",
      "dis_loss: [14.845286    0.14453125]\n",
      "epoch: 3, srgan_loss:  1148.1447\n",
      "4\n",
      "dis_loss: [3.1234846 0.28125  ]\n",
      "epoch: 4, srgan_loss:  964.4962\n",
      "5\n",
      "dis_loss: [2.135356  0.2734375]\n",
      "epoch: 5, srgan_loss:  55.61962\n",
      "6\n",
      "dis_loss: [1.0044922 0.4140625]\n",
      "epoch: 6, srgan_loss:  74.91776\n",
      "7\n",
      "dis_loss: [0.6842003  0.37109375]\n",
      "epoch: 7, srgan_loss:  78.736694\n",
      "8\n",
      "dis_loss: [0.58363533 0.55078125]\n",
      "epoch: 8, srgan_loss:  64.71776\n",
      "9\n",
      "dis_loss: [0.59367627 0.5234375 ]\n",
      "epoch: 9, srgan_loss:  84.11745\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define input shape\n",
    "input_shape = (64, 64, 3)\n",
    "scale = 4\n",
    "\n",
    "def SRGAN_TRAIN(input_shape, scale, n_epoch, batch_size):\n",
    "    # Training phase\n",
    "    output_shape = (input_shape[0]*scale, input_shape[1]*scale, input_shape[2])\n",
    "    # read inputs\n",
    "    lr_patch = Input(input_shape)\n",
    "    hr_patch = Input(output_shape)\n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer = Adam(0.002, 0.5)\n",
    "    \n",
    "    # building phase\n",
    "    # use vgg feature: no training\n",
    "    vgg_feature = VGG_FEATURES(output_shape)\n",
    "    vgg_feature.trainable = False\n",
    "    vgg_feature.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    print(\"vgg feature (out metric names):\", vgg_feature.metrics_names)\n",
    "    # create the discriminator model\n",
    "    srgan_dis = SRGAN_DIS(output_shape)\n",
    "    srgan_dis.compile(loss='mse', optimizer=optimizer, metrics=['accuracy']) \n",
    "    print(\"srgan dis (out metric names):\", srgan_dis.metrics_names)\n",
    "    # create srgan complete model\n",
    "    hr_patch = Input(output_shape)\n",
    "    lr_patch = Input(input_shape)\n",
    "    \n",
    "    srgan_gen = SRGAN_GEN(input_shape)\n",
    "    pred_hr = srgan_gen(lr_patch)\n",
    "    pred_features = vgg_feature(pred_hr)\n",
    "    srgan_dis.trainable = False\n",
    "    real = srgan_dis(pred_hr)\n",
    "    \n",
    "    srgan_complete = Model([lr_patch, hr_patch], [real, pred_features])\n",
    "    srgan_complete.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=optimizer)\n",
    "    print(\"srgan complete (out metric names):\", srgan_complete.metrics_names)\n",
    "    \n",
    "    for n in range(n_epoch):\n",
    "        print(n)\n",
    "        # prepare dataset function\n",
    "        hr_patch, lr_patch = PREPARE_DATASET(batch_size);\n",
    "        pred_hr = srgan_gen.predict(lr_patch)\n",
    "        # Train the discriminator\n",
    "        real = np.ones((batch_size,) + (8, 8, 1))\n",
    "        fake = np.zeros((batch_size,) + (8, 8, 1)) \n",
    "        dis_loss_real = srgan_dis.train_on_batch(hr_patch, real)\n",
    "        dis_loss_pred = srgan_dis.train_on_batch(pred_hr, fake)\n",
    "        dis_loss = 0.5 * np.add(dis_loss_real, dis_loss_pred)\n",
    "        print(\"dis_loss:\", (dis_loss))\n",
    "        # Train generator\n",
    "        real = np.ones((batch_size,) + (8, 8, 1)) # why (8, 8, 1) patch must be size 1 ?\n",
    "        real_features = vgg_feature.predict(hr_patch)\n",
    "        srgan_loss = srgan_complete.train_on_batch([lr_patch, hr_patch], [real, real_features])\n",
    "        # print epoch, srgan loss\n",
    "        if n % 1 == 0:\n",
    "            print(\"epoch: %s, srgan_loss: \" %(n), (srgan_loss[0]))\n",
    "            \n",
    "SRGAN_TRAIN(input_shape, scale=4, n_epoch=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
